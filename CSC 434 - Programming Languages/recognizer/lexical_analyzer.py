class Lexical_Analyzer:
    """
    Used to tokenize a small faux programming language.
    """

    def __init__(self):
        self.state = 0
        self.table = []
        self.tokens = []


    def read_table(self, filename):
        table = []
        token_table = []
        table_file = open(filename, 'r')

        line = table_file.readline()
        while line != '\n':
            line = line.rstrip('\n').split(' ')
            row = []
            for number in line:
                row.append(int(number))

            table.append(row)
            line = table_file.readline()

        line = table_file.readline()
        while line:
            line = line.rstrip('\n').split(' ')
            row = []
            for token in line:
                row.append(token)

            token_table.append(row)
            line = table_file.readline()

        self.table = table
        self.tokens = token_table


    def tokenize(self, statement):
        """
        Accepts a program statement and returns a string containing
        the tokens that are generated by that statement.
        :param statement: The statement to be tokenized.
        :return: The statement's tokenized form.
        """

        if self.table == []:
            print("Error: You must first read in a state transition table with the 'read_table' method.")
            return

        token_string = ''

        for char in statement:
            lookup_row = self.table[self.state]

            if ord(char) >= ord('a') and ord(char) <= ord('z'):
                index = 1
            elif ord(char) >= ord('0') and ord(char) <= ord('9'):
                index = 2
            elif char in '+-*/':
                index = 3
            elif char == '=':
                index = 4
            elif char == ' ':
                index = 5
            else:
                index = 6

            next_state = lookup_row[index]

            if next_state == 5:
                self.state = 0
                token_string += 'error'
                return token_string

            if self.state != next_state and next_state != 0:
                token_string += self.tokens[self.state][next_state] + ' '
                self.state = next_state



        self.state = 0
        return token_string


if __name__ == '__main__':
    analyzer = Lexical_Analyzer()
    analyzer.read_table('state_transition_table')

    code_file = open('examples.txt', 'r')
    for line in code_file:
        line = line.rstrip('\n')
        print(line + ' tokenizes as ' + analyzer.tokenize(line))